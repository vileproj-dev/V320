#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced AI Manager
Gerenciador de IA com Gemini direto usando 3 chaves com rotaÃ§Ã£o automÃ¡tica
ZERO SIMULAÃ‡ÃƒO - Apenas modelos reais funcionais
"""

import os
import logging
import asyncio
import json
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

# Import do cliente Gemini direto
from .gemini_direct_client import gemini_direct_client, generate_with_gemini_direct, generate_with_gemini_direct_sync

logger = logging.getLogger(__name__)

class EnhancedAIManager:
    """Gerenciador de IA aprimorado com Gemini direto"""

    def __init__(self):
        """Inicializa o gerenciador aprimorado com Gemini direto"""
        self.gemini_client = gemini_direct_client
        self.search_orchestrator = None
        
        # Importar search orchestrator se disponÃ­vel
        try:
            from .real_search_orchestrator import RealSearchOrchestrator
            self.search_orchestrator = RealSearchOrchestrator()
            logger.info("âœ… Search Orchestrator carregado")
        except ImportError:
            logger.warning("âš ï¸ Search Orchestrator nÃ£o disponÃ­vel")

        logger.info("ğŸ¤– Enhanced AI Manager inicializado com Gemini direto")

    def generate_response(
        self,
        prompt: str,
        model: str = "gemini-2.0-flash-exp",
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Gera resposta sÃ­ncrona usando Gemini direto"""
        try:
            # Usa o cliente Gemini direto
            response = self.gemini_client.generate_content_sync(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # Verifica se response Ã© dict vÃ¡lido
            if response and isinstance(response, dict) and response.get('success'):
                return {
                    'success': True,
                    'content': response.get('content', ''),
                    'model': response.get('model_used', model),
                    'provider': 'gemini_direct',
                    'tokens_used': response.get('tokens_used', 0),
                    'api_key_used': response.get('api_key_used', 'gemini_1')
                }
            else:
                logger.error(f"âŒ Falha na geraÃ§Ã£o de resposta: {response}")
                error_msg = 'Erro desconhecido'
                if isinstance(response, dict):
                    error_msg = response.get('error', 'Erro desconhecido')
                return {
                    'success': False,
                    'content': 'Erro ao gerar resposta',
                    'error': error_msg
                }
                
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o de resposta: {e}")
            return {
                'success': False,
                'content': 'Erro interno ao gerar resposta',
                'error': str(e)
            }

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """
        Gera texto usando Gemini direto
        
        Args:
            prompt: Prompt do usuÃ¡rio
            system_prompt: Prompt do sistema (opcional)
            max_tokens: MÃ¡ximo de tokens (opcional)
            temperature: Temperatura (opcional)
            model_override: Ignorado (sempre usa Gemini)
        
        Returns:
            String com a resposta da IA
        """
        try:
            return await generate_with_gemini_direct(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens or 4000,
                temperature=temperature or 0.7
            )
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"âŒ Erro de conexÃ£o ao gerar texto: {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"âŒ Erro de parÃ¢metros ao gerar texto: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"âŒ Erro inesperado ao gerar texto: {str(e)}")
            raise
    
    def generate_text_sync(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """VersÃ£o sÃ­ncrona da geraÃ§Ã£o de texto"""
        try:
            return generate_with_gemini_direct_sync(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens or 4000,
                temperature=temperature or 0.7
            )
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"âŒ Erro de conexÃ£o ao gerar texto (sync): {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"âŒ Erro de parÃ¢metros ao gerar texto (sync): {str(e)}")
            raise
        except Exception as e:
            logger.error(f"âŒ Erro inesperado ao gerar texto (sync): {str(e)}")
            raise

    async def generate_with_active_search(
        self,
        prompt: str,
        context: str = "",
        session_id: str = None,
        max_search_iterations: int = 3,
        preferred_model: str = None,
        min_processing_time: int = 0
    ) -> str:
        """
        Gera conteÃºdo com busca ativa usando hierarquia OpenRouter
        """
        logger.info(f"ğŸ” Iniciando geraÃ§Ã£o com busca ativa (min_time: {min_processing_time}s)")
        
        # Registrar tempo de inÃ­cio para garantir tempo mÃ­nimo
        start_time = datetime.now()

        # Prepara prompt com instruÃ§Ãµes de busca e contexto
        enhanced_prompt = f"""
{prompt}

CONTEXTO DISPONÃVEL:
{context}

INSTRUÃ‡Ã•ES ESPECIAIS:
- Analise o contexto fornecido detalhadamente
- Busque dados atualizados sobre o mercado brasileiro
- Procure por estatÃ­sticas, tendÃªncias e casos reais
- ForneÃ§a insights profundos baseados nos dados disponÃ­veis
- Use informaÃ§Ãµes reais e atualizadas sempre que possÃ­vel

IMPORTANTE: Gere uma anÃ¡lise completa e profissional baseando-se no contexto fornecido.
"""

        # Sistema prompt para busca ativa
        system_prompt = """VocÃª Ã© um especialista em anÃ¡lise de mercado e tendÃªncias digitais. 
        Sua funÃ§Ã£o Ã© gerar anÃ¡lises profundas e insights valiosos baseados nos dados fornecidos.
        Sempre forneÃ§a informaÃ§Ãµes precisas, atualizadas e acionÃ¡veis.
        Se precisar de informaÃ§Ãµes adicionais, indique claramente quais dados seriam Ãºteis."""

        try:
            # Sempre usar Gemini direto (preferred_model Ã© ignorado)
            logger.info(f"ğŸ¤– Usando Gemini direto (preferÃªncia {preferred_model} ignorada)")
            
            # Gerar resposta usando Gemini direto
            response = await self.generate_text(
                prompt=enhanced_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7
            )
            
            # Garantir tempo mÃ­nimo de processamento se especificado
            if min_processing_time > 0:
                elapsed_time = (datetime.now() - start_time).total_seconds()
                if elapsed_time < min_processing_time:
                    remaining_time = min_processing_time - elapsed_time
                    logger.info(f"â±ï¸ Aguardando {remaining_time:.1f}s para completar tempo mÃ­nimo")
                    await asyncio.sleep(remaining_time)
            
            logger.info("âœ… GeraÃ§Ã£o com busca ativa concluÃ­da")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o com busca ativa: {e}")
            # Fallback simples
            try:
                return await self.generate_text(enhanced_prompt, system_prompt)
            except Exception as e2:
                logger.error(f"âŒ Erro no fallback: {e2}")
                raise

    async def analyze_content(
        self,
        content: str,
        analysis_type: str = "comprehensive",
        target_audience: str = "general",
        model_preference: str = None
    ) -> str:
        """
        Analisa conteÃºdo usando hierarquia OpenRouter
        
        Args:
            content: ConteÃºdo para anÃ¡lise
            analysis_type: Tipo de anÃ¡lise (comprehensive, viral, market, etc.)
            target_audience: PÃºblico-alvo
            model_preference: PreferÃªncia de modelo
        
        Returns:
            AnÃ¡lise detalhada do conteÃºdo
        """
        system_prompt = f"""VocÃª Ã© um especialista em anÃ¡lise de conteÃºdo digital e marketing.
        Sua funÃ§Ã£o Ã© analisar conteÃºdo de forma {analysis_type} para o pÃºblico {target_audience}.
        ForneÃ§a insights acionÃ¡veis, tendÃªncias identificadas e recomendaÃ§Ãµes estratÃ©gicas."""
        
        analysis_prompt = f"""
Analise o seguinte conteÃºdo de forma {analysis_type}:

CONTEÃšDO:
{content}

PÃšBLICO-ALVO: {target_audience}

FORNEÃ‡A:
1. AnÃ¡lise detalhada do conteÃºdo
2. Pontos fortes e fracos identificados
3. Potencial viral e engajamento
4. RecomendaÃ§Ãµes de melhoria
5. EstratÃ©gias de distribuiÃ§Ã£o
6. Insights de mercado relevantes

Seja especÃ­fico, prÃ¡tico e acionÃ¡vel em suas recomendaÃ§Ãµes.
"""
        
        try:
            return await self.generate_text(
                prompt=analysis_prompt,
                system_prompt=system_prompt,
                max_tokens=3000,
                temperature=0.7,
                model_override=model_preference
            )
        except Exception as e:
            logger.error(f"âŒ Erro na anÃ¡lise de conteÃºdo: {e}")
            raise

    async def generate_insights(
        self,
        data: Dict[str, Any],
        insight_type: str = "market_trends",
        depth: str = "deep"
    ) -> str:
        """
        Gera insights baseados em dados usando hierarquia OpenRouter
        
        Args:
            data: Dados para anÃ¡lise
            insight_type: Tipo de insight desejado
            depth: Profundidade da anÃ¡lise (shallow, medium, deep)
        
        Returns:
            Insights gerados
        """
        system_prompt = f"""VocÃª Ã© um analista de dados especializado em {insight_type}.
        Sua funÃ§Ã£o Ã© gerar insights {depth} baseados nos dados fornecidos.
        Sempre forneÃ§a anÃ¡lises precisas, tendÃªncias identificadas e recomendaÃ§Ãµes acionÃ¡veis."""
        
        data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        insights_prompt = f"""
Analise os seguintes dados e gere insights {depth} sobre {insight_type}:

DADOS:
{data_str}

FORNEÃ‡A:
1. Principais tendÃªncias identificadas
2. PadrÃµes e correlaÃ§Ãµes importantes
3. Oportunidades de mercado
4. Riscos e desafios
5. RecomendaÃ§Ãµes estratÃ©gicas
6. PrevisÃµes baseadas nos dados

Seja especÃ­fico, use nÃºmeros quando relevante e forneÃ§a insights acionÃ¡veis.
"""
        
        try:
            return await self.generate_text(
                prompt=insights_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.6
            )
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o de insights: {e}")
            raise

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do gerenciador"""
        return {
            "gemini_status": self.gemini_client.get_status(),
            "search_orchestrator_available": self.search_orchestrator is not None,
            "timestamp": datetime.now().isoformat()
        }

    def reset_failed_models(self):
        """Reseta estatÃ­sticas do cliente Gemini"""
        self.gemini_client.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "key_rotations": 0,
            "last_used_key": None
        }
        logger.info("âœ… EstatÃ­sticas Gemini resetadas")

# InstÃ¢ncia global para uso em todo o projeto
enhanced_ai_manager = EnhancedAIManager()

# FunÃ§Ãµes de conveniÃªncia para uso direto
async def generate_ai_text(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """FunÃ§Ã£o de conveniÃªncia para geraÃ§Ã£o de texto"""
    return await enhanced_ai_manager.generate_text(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )

def generate_ai_text_sync(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """FunÃ§Ã£o de conveniÃªncia sÃ­ncrona para geraÃ§Ã£o de texto"""
    return enhanced_ai_manager.generate_text_sync(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )

if __name__ == "__main__":
    # Teste bÃ¡sico
    async def test():
        try:
            manager = EnhancedAIManager()
            
            response = await manager.generate_text(
                prompt="Explique brevemente o que Ã© inteligÃªncia artificial",
                system_prompt="VocÃª Ã© um especialista em tecnologia"
            )
            print(f"Resposta: {response}")
            
            # Status
            status = manager.get_status()
            print(f"Status: {json.dumps(status, indent=2, default=str)}")
            
        except Exception as e:
            print(f"Erro no teste: {e}")
    
    asyncio.run(test())