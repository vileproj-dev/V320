#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced AI Manager
Gerenciador de IA com Gemini direto usando 3 chaves com rota√ß√£o autom√°tica
ZERO SIMULA√á√ÉO - Apenas modelos reais funcionais
"""

import os
import logging
import asyncio
import json
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

# Import do cliente Gemini direto
from .gemini_direct_client import gemini_direct_client, generate_with_gemini_direct, generate_with_gemini_direct_sync

logger = logging.getLogger(__name__)

class EnhancedAIManager:
    """Gerenciador de IA aprimorado com Gemini direto"""

    def __init__(self):
        """Inicializa o gerenciador aprimorado com Gemini direto"""
        self.gemini_client = gemini_direct_client
        self.search_orchestrator = None
        
        # Importar search orchestrator se dispon√≠vel
        try:
            from .real_search_orchestrator import RealSearchOrchestrator
            self.search_orchestrator = RealSearchOrchestrator()
            logger.info("‚úÖ Search Orchestrator carregado")
        except ImportError:
            logger.warning("‚ö†Ô∏è Search Orchestrator n√£o dispon√≠vel")

        logger.info("ü§ñ Enhanced AI Manager inicializado com Gemini direto")

    def generate_response(
        self,
        prompt: str,
        model: str = "gemini-2.0-flash-exp",
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Gera resposta s√≠ncrona usando Gemini direto"""
        try:
            # Usa o cliente Gemini direto
            response = self.gemini_client.generate_content_sync(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # Verifica se response √© dict v√°lido
            if response and isinstance(response, dict) and response.get('success'):
                return {
                    'success': True,
                    'content': response.get('content', ''),
                    'model': response.get('model_used', model),
                    'provider': 'gemini_direct',
                    'tokens_used': response.get('tokens_used', 0),
                    'api_key_used': response.get('api_key_used', 'gemini_1')
                }
            else:
                logger.error(f"‚ùå Falha na gera√ß√£o de resposta: {response}")
                error_msg = 'Erro desconhecido'
                if isinstance(response, dict):
                    error_msg = response.get('error', 'Erro desconhecido')
                return {
                    'success': False,
                    'content': 'Erro ao gerar resposta',
                    'error': error_msg
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de resposta: {e}")
            return {
                'success': False,
                'content': 'Erro interno ao gerar resposta',
                'error': str(e)
            }

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """
        Gera texto usando Gemini direto
        
        Args:
            prompt: Prompt do usu√°rio
            system_prompt: Prompt do sistema (opcional)
            max_tokens: M√°ximo de tokens (opcional)
            temperature: Temperatura (opcional)
            model_override: Ignorado (sempre usa Gemini)
        
        Returns:
            String com a resposta da IA
        """
        try:
            return await generate_with_gemini_direct(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens or 4000,
                temperature=temperature or 0.7
            )
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"‚ùå Erro de conex√£o ao gerar texto: {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"‚ùå Erro de par√¢metros ao gerar texto: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao gerar texto: {str(e)}")
            raise
    
    def generate_text_sync(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """Vers√£o s√≠ncrona da gera√ß√£o de texto"""
        try:
            return generate_with_gemini_direct_sync(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens or 4000,
                temperature=temperature or 0.7
            )
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"‚ùå Erro de conex√£o ao gerar texto (sync): {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"‚ùå Erro de par√¢metros ao gerar texto (sync): {str(e)}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao gerar texto (sync): {str(e)}")
            raise

    async def generate_with_active_search(
        self,
        prompt: str,
        context: str = "",
        session_id: str = None,
        max_search_iterations: int = 3,
        preferred_model: str = None,
        min_processing_time: int = 0
    ) -> str:
        """
        Gera conte√∫do com busca ativa usando hierarquia OpenRouter
        """
        logger.info(f"üîç Iniciando gera√ß√£o com busca ativa (min_time: {min_processing_time}s)")
        
        # Registrar tempo de in√≠cio para garantir tempo m√≠nimo
        start_time = datetime.now()

        # Prepara prompt com instru√ß√µes de busca e contexto
        enhanced_prompt = f"""
{prompt}

CONTEXTO DISPON√çVEL:
{context}

INSTRU√á√ïES ESPECIAIS:
- Analise o contexto fornecido detalhadamente
- Busque dados atualizados sobre o mercado brasileiro
- Procure por estat√≠sticas, tend√™ncias e casos reais
- Forne√ßa insights profundos baseados nos dados dispon√≠veis
- Use informa√ß√µes reais e atualizadas sempre que poss√≠vel

IMPORTANTE: Gere uma an√°lise completa e profissional baseando-se no contexto fornecido.
"""

        # Sistema prompt para busca ativa
        system_prompt = """Voc√™ √© um especialista em an√°lise de mercado e tend√™ncias digitais. 
        Sua fun√ß√£o √© gerar an√°lises profundas e insights valiosos baseados nos dados fornecidos.
        Sempre forne√ßa informa√ß√µes precisas, atualizadas e acion√°veis.
        Se precisar de informa√ß√µes adicionais, indique claramente quais dados seriam √∫teis."""

        try:
            # Sempre usar Gemini direto (preferred_model √© ignorado)
            logger.info(f"ü§ñ Usando Gemini direto (prefer√™ncia {preferred_model} ignorada)")
            
            # Gerar resposta usando Gemini direto
            response = await self.generate_text(
                prompt=enhanced_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7
            )
            
            # Garantir tempo m√≠nimo de processamento se especificado
            if min_processing_time > 0:
                elapsed_time = (datetime.now() - start_time).total_seconds()
                if elapsed_time < min_processing_time:
                    remaining_time = min_processing_time - elapsed_time
                    logger.info(f"‚è±Ô∏è Aguardando {remaining_time:.1f}s para completar tempo m√≠nimo")
                    await asyncio.sleep(remaining_time)
            
            logger.info("‚úÖ Gera√ß√£o com busca ativa conclu√≠da")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o com busca ativa: {e}")
            # Fallback simples
            try:
                return await self.generate_text(enhanced_prompt, system_prompt)
            except Exception as e2:
                logger.error(f"‚ùå Erro no fallback: {e2}")
                raise

    async def analyze_content(
        self,
        content: str,
        analysis_type: str = "comprehensive",
        target_audience: str = "general",
        model_preference: str = None
    ) -> str:
        """
        Analisa conte√∫do usando hierarquia OpenRouter
        
        Args:
            content: Conte√∫do para an√°lise
            analysis_type: Tipo de an√°lise (comprehensive, viral, market, etc.)
            target_audience: P√∫blico-alvo
            model_preference: Prefer√™ncia de modelo
        
        Returns:
            An√°lise detalhada do conte√∫do
        """
        system_prompt = f"""Voc√™ √© um especialista em an√°lise de conte√∫do digital e marketing.
        Sua fun√ß√£o √© analisar conte√∫do de forma {analysis_type} para o p√∫blico {target_audience}.
        Forne√ßa insights acion√°veis, tend√™ncias identificadas e recomenda√ß√µes estrat√©gicas."""
        
        analysis_prompt = f"""
Analise o seguinte conte√∫do de forma {analysis_type}:

CONTE√öDO:
{content}

P√öBLICO-ALVO: {target_audience}

FORNE√áA:
1. An√°lise detalhada do conte√∫do
2. Pontos fortes e fracos identificados
3. Potencial viral e engajamento
4. Recomenda√ß√µes de melhoria
5. Estrat√©gias de distribui√ß√£o
6. Insights de mercado relevantes

Seja espec√≠fico, pr√°tico e acion√°vel em suas recomenda√ß√µes.
"""
        
        try:
            return await self.generate_text(
                prompt=analysis_prompt,
                system_prompt=system_prompt,
                max_tokens=3000,
                temperature=0.7,
                model_override=model_preference
            )
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de conte√∫do: {e}")
            raise

    async def generate_insights(
        self,
        data: Dict[str, Any],
        insight_type: str = "market_trends",
        depth: str = "deep"
    ) -> str:
        """
        Gera insights baseados em dados usando hierarquia OpenRouter
        
        Args:
            data: Dados para an√°lise
            insight_type: Tipo de insight desejado
            depth: Profundidade da an√°lise (shallow, medium, deep)
        
        Returns:
            Insights gerados
        """
        system_prompt = f"""Voc√™ √© um analista de dados especializado em {insight_type}.
        Sua fun√ß√£o √© gerar insights {depth} baseados nos dados fornecidos.
        Sempre forne√ßa an√°lises precisas, tend√™ncias identificadas e recomenda√ß√µes acion√°veis."""
        
        data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        insights_prompt = f"""
Analise os seguintes dados e gere insights {depth} sobre {insight_type}:

DADOS:
{data_str}

FORNE√áA:
1. Principais tend√™ncias identificadas
2. Padr√µes e correla√ß√µes importantes
3. Oportunidades de mercado
4. Riscos e desafios
5. Recomenda√ß√µes estrat√©gicas
6. Previs√µes baseadas nos dados

Seja espec√≠fico, use n√∫meros quando relevante e forne√ßa insights acion√°veis.
"""
        
        try:
            return await self.generate_text(
                prompt=insights_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.6
            )
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de insights: {e}")
            raise

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do gerenciador"""
        return {
            "gemini_status": self.gemini_client.get_status(),
            "search_orchestrator_available": self.search_orchestrator is not None,
            "timestamp": datetime.now().isoformat()
        }

    def reset_failed_models(self):
        """Reseta estat√≠sticas do cliente Gemini"""
        self.gemini_client.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "key_rotations": 0,
            "last_used_key": None
        }
        logger.info("‚úÖ Estat√≠sticas Gemini resetadas")

# Inst√¢ncia global para uso em todo o projeto
enhanced_ai_manager = EnhancedAIManager()

# Fun√ß√µes de conveni√™ncia para uso direto
async def generate_ai_text(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """Fun√ß√£o de conveni√™ncia para gera√ß√£o de texto"""
    return await enhanced_ai_manager.generate_text(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )

def generate_ai_text_sync(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """Fun√ß√£o de conveni√™ncia s√≠ncrona para gera√ß√£o de texto"""
    return enhanced_ai_manager.generate_text_sync(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )

if __name__ == "__main__":
    # Teste b√°sico
    async def test():
        try:
            manager = EnhancedAIManager()
            
            response = await manager.generate_text(
                prompt="Explique brevemente o que √© intelig√™ncia artificial",
                system_prompt="Voc√™ √© um especialista em tecnologia"
            )
            print(f"Resposta: {response}")
            
            # Status
            status = manager.get_status()
            print(f"Status: {json.dumps(status, indent=2, default=str)}")
            
        except Exception as e:
            print(f"Erro no teste: {e}")
    
    asyncio.run(test())