#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Predictive Analytics Service
Servi√ßo Centralizado de An√°lise Preditiva Ultra-Avan√ßado
"""

import os
import logging
import json
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path

# Import do engine existente
from engine.predictive_analytics_engine import PredictiveAnalyticsEngine

logger = logging.getLogger(__name__)

class PredictiveAnalyticsService:
    """
    Servi√ßo centralizado que encapsula o PredictiveAnalyticsEngine
    e exp√µe funcionalidades espec√≠ficas para consumo por outros m√≥dulos.
    """

    def __init__(self):
        """Inicializa o servi√ßo de an√°lise preditiva"""
        try:
            self.engine = PredictiveAnalyticsEngine()
            self.available = True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao inicializar engine preditivo: {e}")
            self.engine = None
            self.available = False
        logger.info("üîÆ Predictive Analytics Service inicializado")

    def is_available(self) -> bool:
        """Verifica se o servi√ßo est√° dispon√≠vel"""
        return self.available and self.engine is not None

    async def analyze_session(self, session_id: str) -> Dict[str, Any]:
        """
        Executa a an√°lise completa de uma sess√£o.

        Args:
            session_id: ID da sess√£o para an√°lise

        Returns:
            Dict com insights preditivos completos
        """
        try:
            logger.info(f"üîÆ Iniciando an√°lise preditiva completa para sess√£o: {session_id}")

            # Chama o m√©todo principal do engine
            insights = await self.engine.analyze_session_data(session_id)

            if insights.get("success", False):
                logger.info(f"‚úÖ An√°lise preditiva conclu√≠da para sess√£o: {session_id}")
            else:
                logger.warning(f"‚ö†Ô∏è An√°lise preditiva com problemas para sess√£o: {session_id}")

            return insights

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise preditiva da sess√£o {session_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    async def analyze_content_chunk(self, text_content: str) -> Dict[str, Any]:
        """
        Analisa um pequeno trecho de texto para insights r√°pidos.

        Args:
            text_content: Texto para an√°lise

        Returns:
            Dict com insights do conte√∫do
        """
        try:
            logger.info("üß† Analisando chunk de conte√∫do...")

            # An√°lise simplificada para chunks de texto
            insights = await self._analyze_text_chunk_simple(text_content)

            return {
                "success": True,
                "content_insights": insights,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de chunk: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _analyze_text_chunk_simple(self, text_content: str) -> Dict[str, Any]:
        """
        An√°lise simplificada de um chunk de texto.

        Args:
            text_content: Texto para an√°lise

        Returns:
            Dict com insights b√°sicos
        """
        insights = {
            "word_count": 0,
            "key_entities": [],
            "sentiment_score": 0.0,
            "topics": [],
            "quality_indicators": {}
        }

        if not text_content or not text_content.strip():
            return insights

        # Contagem b√°sica
        words = text_content.split()
        insights["word_count"] = len(words)

        # An√°lise com SpaCy se dispon√≠vel
        if hasattr(self.engine, 'nlp_model') and self.engine.nlp_model:
            try:
                doc = self.engine.nlp_model(text_content[:1000])  # Limita para performance

                # Extrai entidades
                entities = [(ent.text, ent.label_) for ent in doc.ents]
                insights["key_entities"] = entities[:10]  # Top 10

                # An√°lise de qualidade
                insights["quality_indicators"] = {
                    "has_entities": len(entities) > 0,
                    "entity_diversity": len(set([ent[1] for ent in entities])),
                    "avg_sentence_length": len(text_content) / max(len(list(doc.sents)), 1)
                }

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na an√°lise NLP do chunk: {e}")

        # An√°lise de sentimento se dispon√≠vel
        if hasattr(self.engine, 'sentiment_analyzer') and self.engine.sentiment_analyzer:
            try:
                sentiment = self.engine.sentiment_analyzer.polarity_scores(text_content)
                insights["sentiment_score"] = sentiment.get("compound", 0.0)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na an√°lise de sentimento: {e}")

        return insights

    def get_content_quality_score(self, text_data: str) -> float:
        """
        Retorna um score num√©rico (0-100) representando a qualidade/relev√¢ncia do conte√∫do.

        Args:
            text_data: Texto para avalia√ß√£o

        Returns:
            Score de qualidade (0-100)
        """
        try:
            if not text_data or not text_data.strip():
                return 0.0

            score = 0.0

            # Crit√©rio 1: Comprimento (20 pontos)
            length = len(text_data.strip())
            if length > 1000:
                score += 20
            elif length > 500:
                score += 15
            elif length > 200:
                score += 10
            elif length > 50:
                score += 5

            # Crit√©rio 2: Densidade de informa√ß√µes (30 pontos)
            words = text_data.split()
            if len(words) > 100:
                score += 30
            elif len(words) > 50:
                score += 20
            elif len(words) > 20:
                score += 10

            # Crit√©rio 3: Presen√ßa de entidades/dados estruturados (25 pontos)
            # Busca por padr√µes que indicam dados estruturados
            structured_patterns = [
                r'\d+%',  # Percentuais
                r'R\$\s*\d+',  # Valores monet√°rios
                r'\d{4}',  # Anos
                r'@\w+',  # Men√ß√µes
                r'#\w+',  # Hashtags
                r'www\.\w+',  # URLs
                r'\d+\s*(mil|milh√£o|bilh√£o)',  # N√∫meros grandes
            ]

            import re
            pattern_matches = 0
            for pattern in structured_patterns:
                if re.search(pattern, text_data, re.IGNORECASE):
                    pattern_matches += 1

            score += min(pattern_matches * 5, 25)

            # Crit√©rio 4: Diversidade lexical (15 pontos)
            unique_words = set(word.lower() for word in words if len(word) > 3)
            if len(words) > 0:
                diversity_ratio = len(unique_words) / len(words)
                score += diversity_ratio * 15

            # Crit√©rio 5: Aus√™ncia de spam/conte√∫do irrelevante (10 pontos)
            spam_indicators = ['lorem ipsum', 'placeholder', 'exemplo', 'teste', 'sample']
            has_spam = any(indicator in text_data.lower() for indicator in spam_indicators)
            if not has_spam:
                score += 10

            # Normaliza para 0-100
            final_score = min(max(score, 0), 100)

            logger.debug(f"üìä Score de qualidade calculado: {final_score:.1f}")
            return final_score

        except Exception as e:
            logger.error(f"‚ùå Erro no c√°lculo de score de qualidade: {e}")
            return 0.0

    async def generate_recommendations(self, insights_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Gera recomenda√ß√µes estrat√©gicas a partir de um conjunto de insights.

        Args:
            insights_data: Dados de insights para gerar recomenda√ß√µes

        Returns:
            Dict com recomenda√ß√µes estrat√©gicas
        """
        try:
            logger.info("üí° Gerando recomenda√ß√µes estrat√©gicas...")

            recommendations = {
                "strategic_recommendations": [],
                "quick_wins": [],
                "content_strategy": [],
                "competitive_actions": [],
                "risk_mitigation": [],
                "timestamp": datetime.now().isoformat()
            }

            # Analisa insights para gerar recomenda√ß√µes espec√≠ficas
            if "textual_insights" in insights_data:
                textual = insights_data["textual_insights"]

                # Recomenda√ß√µes baseadas em entidades encontradas
                if "entities_found" in textual:
                    entities = textual["entities_found"]
                    if entities:
                        recommendations["strategic_recommendations"].append({
                            "category": "Entidades Relevantes",
                            "action": f"Focar em {len(entities)} entidades-chave identificadas na an√°lise",
                            "priority": "alta",
                            "entities": list(entities.keys())[:5]
                        })

                # Quick wins baseados em qualidade de conte√∫do
                if "content_quality_scores" in textual:
                    scores = textual["content_quality_scores"]
                    high_quality = [k for k, v in scores.items() if v > 80]
                    if high_quality:
                        recommendations["quick_wins"].append({
                            "action": f"Replicar estrat√©gias dos {len(high_quality)} conte√∫dos de alta qualidade identificados",
                            "impact": "m√©dio",
                            "effort": "baixo"
                        })

            # Recomenda√ß√µes baseadas em tend√™ncias temporais
            if "temporal_trends" in insights_data:
                temporal = insights_data["temporal_trends"]
                if "growth_rates" in temporal and temporal["growth_rates"] != "Depende de timestamps coletados pelos crawlers.":
                    recommendations["content_strategy"].append({
                        "category": "Timing",
                        "action": "Otimizar hor√°rios de publica√ß√£o baseado em tend√™ncias identificadas",
                        "data_source": "an√°lise temporal"
                    })

            # Recomenda√ß√µes baseadas em an√°lise visual
            if "visual_insights" in insights_data:
                visual = insights_data["visual_insights"]
                if visual.get("screenshots_processed", 0) > 0:
                    recommendations["competitive_actions"].append({
                        "category": "An√°lise Visual",
                        "action": f"Analisar {visual['screenshots_processed']} elementos visuais dos concorrentes",
                        "keywords_found": visual.get("keywords_from_images", [])[:10]
                    })

            logger.info("‚úÖ Recomenda√ß√µes estrat√©gicas geradas")
            return recommendations

        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de recomenda√ß√µes: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def refine_search_queries(self, current_context: str, existing_results: List[Dict]) -> List[str]:
        """
        Sugere novas queries de busca para aprimorar a coleta.

        Args:
            current_context: Contexto atual da busca
            existing_results: Resultados j√° obtidos

        Returns:
            Lista de queries refinadas
        """
        try:
            logger.info("üîç Refinando queries de busca...")

            refined_queries = []

            # Analisa resultados existentes para identificar gaps
            if existing_results:
                # Extrai termos-chave dos resultados existentes
                all_text = " ".join([
                    result.get("title", "") + " " + result.get("snippet", "") + " " + result.get("description", "")
                    for result in existing_results
                ])

                # Identifica termos frequentes
                words = all_text.lower().split()
                from collections import Counter
                common_terms = [term for term, count in Counter(words).most_common(10) if len(term) > 3]

                # Gera queries refinadas baseadas no contexto
                base_context = current_context.lower()

                # Queries espec√≠ficas por categoria
                if "marketing" in base_context or "publicidade" in base_context:
                    refined_queries.extend([
                        f"{current_context} estrat√©gias digitais 2024",
                        f"{current_context} ROI campanhas",
                        f"{current_context} m√©tricas engajamento"
                    ])

                if "concorrente" in base_context or "competidor" in base_context:
                    refined_queries.extend([
                        f"{current_context} an√°lise SWOT",
                        f"{current_context} posicionamento mercado",
                        f"{current_context} diferenciais competitivos"
                    ])

                # Combina com termos encontrados
                for term in common_terms[:3]:
                    refined_queries.append(f"{current_context} {term} tend√™ncias")

            # Remove duplicatas e limita
            refined_queries = list(set(refined_queries))[:8]

            logger.info(f"‚úÖ {len(refined_queries)} queries refinadas geradas")
            return refined_queries

        except Exception as e:
            logger.error(f"‚ùå Erro no refinamento de queries: {e}")
            return [current_context]  # Retorna query original em caso de erro

    async def analyze_initial_data(self, dados_coletados: List[Dict]) -> Dict[str, Any]:
        """
        Analisa dados iniciais coletados para fornecer insights preliminares.

        Args:
            dados_coletados: Lista de dados coletados na fase inicial

        Returns:
            Dict com insights iniciais
        """
        try:
            logger.info("üìä Analisando dados iniciais...")

            insights_iniciais = {
                "data_quality_assessment": {},
                "content_gaps": [],
                "priority_areas": [],
                "next_steps_suggestions": [],
                "timestamp": datetime.now().isoformat()
            }

            if not dados_coletados:
                insights_iniciais["content_gaps"].append("Nenhum dado coletado na fase inicial")
                return insights_iniciais

            # Avalia qualidade dos dados coletados
            total_content = 0
            quality_scores = []

            for item in dados_coletados:
                if isinstance(item, dict) and "data" in item:
                    content = str(item["data"])
                    score = self.get_content_quality_score(content)
                    quality_scores.append(score)
                    total_content += len(content)

            if quality_scores:
                avg_quality = sum(quality_scores) / len(quality_scores)
                insights_iniciais["data_quality_assessment"] = {
                    "average_quality_score": avg_quality,
                    "total_items": len(dados_coletados),
                    "total_content_length": total_content,
                    "quality_distribution": {
                        "high_quality": len([s for s in quality_scores if s > 80]),
                        "medium_quality": len([s for s in quality_scores if 50 <= s <= 80]),
                        "low_quality": len([s for s in quality_scores if s < 50])
                    }
                }

                # Sugere pr√≥ximos passos baseado na qualidade
                if avg_quality < 60:
                    insights_iniciais["next_steps_suggestions"].append({
                        "action": "Expandir coleta de dados",
                        "reason": "Qualidade m√©dia dos dados abaixo do ideal",
                        "priority": "alta"
                    })

                if avg_quality > 80:
                    insights_iniciais["next_steps_suggestions"].append({
                        "action": "Focar em an√°lise profunda",
                        "reason": "Dados de alta qualidade dispon√≠veis",
                        "priority": "m√©dia"
                    })

            # Identifica √°reas priorit√°rias
            content_types = {}
            for item in dados_coletados:
                if isinstance(item, dict):
                    # Classifica tipo de conte√∫do
                    content = str(item.get("data", ""))
                    if "instagram" in content.lower() or "facebook" in content.lower():
                        content_types["social_media"] = content_types.get("social_media", 0) + 1
                    elif "youtube" in content.lower():
                        content_types["video_content"] = content_types.get("video_content", 0) + 1
                    elif "site" in content.lower() or "website" in content.lower():
                        content_types["web_content"] = content_types.get("web_content", 0) + 1

            # Prioriza √°reas com mais dados
            if content_types:
                sorted_types = sorted(content_types.items(), key=lambda x: x[1], reverse=True)
                insights_iniciais["priority_areas"] = [
                    {"area": area, "data_points": count, "priority": "alta" if i == 0 else "m√©dia"}
                    for i, (area, count) in enumerate(sorted_types[:3])
                ]

            logger.info("‚úÖ An√°lise de dados iniciais conclu√≠da")
            return insights_iniciais

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de dados iniciais: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# Inst√¢ncia global do servi√ßo
predictive_analytics_service = PredictiveAnalyticsService()