#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced Synthesis Engine
Motor de s√≠ntese aprimorado com busca ativa e an√°lise profunda
"""

import os
import logging
import json
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedSynthesisEngine:
    """Motor de s√≠ntese aprimorado com IA e busca ativa"""

    def __init__(self):
        """Inicializa o motor de s√≠ntese"""
        self.synthesis_prompts = self._load_enhanced_prompts()
        self.ai_manager = None
        self._initialize_ai_manager()
        
        logger.info("üß† Enhanced Synthesis Engine inicializado")

    def _initialize_ai_manager(self):
        """Inicializa o gerenciador de IA"""
        try:
            from services.enhanced_ai_manager import enhanced_ai_manager
            self.ai_manager = enhanced_ai_manager
            logger.info("‚úÖ AI Manager conectado ao Synthesis Engine")
        except ImportError:
            logger.error("‚ùå Enhanced AI Manager n√£o dispon√≠vel")

    def _load_enhanced_prompts(self) -> Dict[str, str]:
        """Carrega prompts aprimorados para s√≠ntese"""
        return {
            'master_synthesis': """
# VOC√ä √â O ANALISTA ESTRAT√âGICO MESTRE - S√çNTESE ULTRA-PROFUNDA

Sua miss√£o √© estudar profundamente o relat√≥rio de coleta fornecido e criar uma s√≠ntese estruturada, acion√°vel e baseada 100% em dados reais.

## TEMPO M√çNIMO DE ESPECIALIZA√á√ÉO: 5 MINUTOS
Voc√™ deve dedicar NO M√çNIMO 5 minutos se especializando no tema fornecido, fazendo m√∫ltiplas buscas e an√°lises profundas antes de gerar a s√≠ntese final.

## INSTRU√á√ïES CR√çTICAS:

1. **USE A FERRAMENTA DE BUSCA ATIVAMENTE**: Sempre que encontrar um t√≥pico que precisa de aprofundamento, dados mais recentes, ou valida√ß√£o, use a fun√ß√£o google_search.

2. **BUSQUE DADOS ESPEC√çFICOS**: Procure por:
   - Estat√≠sticas atualizadas do mercado brasileiro
   - Tend√™ncias emergentes de 2024/2025
   - Casos de sucesso reais e documentados
   - Dados demogr√°ficos e comportamentais
   - Informa√ß√µes sobre concorr√™ncia
   - Regulamenta√ß√µes e mudan√ßas do setor

3. **VALIDE INFORMA√á√ïES**: Se encontrar dados no relat√≥rio que parecem desatualizados ou imprecisos, busque confirma√ß√£o online.

4. **ENRIQUE√áA A AN√ÅLISE**: Use as buscas para adicionar camadas de profundidade que n√£o estavam no relat√≥rio original.

## ESTRUTURA OBRIGAT√ìRIA DO JSON DE RESPOSTA:

```json
{
  "insights_principais": [
    "Lista de 15-20 insights principais extra√≠dos e validados com busca"
  ],
  "oportunidades_identificadas": [
    "Lista de 10-15 oportunidades de mercado descobertas"
  ],
  "publico_alvo_refinado": {
    "demografia_detalhada": {
      "idade_predominante": "Faixa et√°ria espec√≠fica baseada em dados reais",
      "genero_distribuicao": "Distribui√ß√£o por g√™nero com percentuais",
      "renda_familiar": "Faixa de renda com dados do IBGE/pesquisas",
      "escolaridade": "N√≠vel educacional predominante",
      "localizacao_geografica": "Regi√µes de maior concentra√ß√£o",
      "estado_civil": "Distribui√ß√£o por estado civil",
      "tamanho_familia": "Composi√ß√£o familiar t√≠pica"
    },
    "psicografia_profunda": {
      "valores_principais": "Valores que guiam decis√µes",
      "estilo_vida": "Como vivem e se comportam",
      "personalidade_dominante": "Tra√ßos de personalidade marcantes",
      "motivacoes_compra": "O que realmente os motiva a comprar",
      "influenciadores": "Quem os influencia nas decis√µes",
      "canais_informacao": "Onde buscam informa√ß√µes",
      "habitos_consumo": "Padr√µes de consumo identificados"
    },
    "comportamentos_digitais": {
      "plataformas_ativas": "Onde est√£o mais ativos online",
      "horarios_pico": "Quando est√£o mais ativos",
      "tipos_conteudo_preferido": "Que tipo de conte√∫do consomem",
      "dispositivos_utilizados": "Mobile, desktop, tablet",
      "jornada_digital": "Como navegam online at√© a compra"
    },
    "dores_viscerais_reais": [
      "Lista de 15-20 dores profundas identificadas nos dados reais"
    ],
    "desejos_ardentes_reais": [
      "Lista de 15-20 desejos identificados nos dados reais"
    ],
    "objecoes_reais_identificadas": [
      "Lista de 12-15 obje√ß√µes reais encontradas nos dados"
    ]
  },
  "estrategias_recomendadas": [
    "Lista de 8-12 estrat√©gias espec√≠ficas baseadas nos achados"
  ],
  "pontos_atencao_criticos": [
    "Lista de 6-10 pontos que requerem aten√ß√£o imediata"
  ],
  "dados_mercado_validados": {
    "tamanho_mercado_atual": "Tamanho atual com fonte",
    "crescimento_projetado": "Proje√ß√£o de crescimento com dados",
    "principais_players": "Lista dos principais players identificados",
    "barreiras_entrada": "Principais barreiras identificadas",
    "fatores_sucesso": "Fatores cr√≠ticos de sucesso no mercado",
    "ameacas_identificadas": "Principais amea√ßas ao neg√≥cio",
    "janelas_oportunidade": "Momentos ideais para entrada/expans√£o"
  },
  "tendencias_futuras_validadas": [
    "Lista de tend√™ncias validadas com busca online"
  ],
  "metricas_chave_sugeridas": {
    "kpis_primarios": "KPIs principais para acompanhar",
    "kpis_secundarios": "KPIs de apoio",
    "benchmarks_mercado": "Benchmarks identificados com dados reais",
    "metas_realistas": "Metas baseadas em dados do mercado",
    "frequencia_medicao": "Com que frequ√™ncia medir cada m√©trica"
  },
  "plano_acao_imediato": {
    "primeiros_30_dias": [
      "A√ß√µes espec√≠ficas para os primeiros 30 dias"
    ],
    "proximos_90_dias": [
      "A√ß√µes para os pr√≥ximos 90 dias"
    ],
    "primeiro_ano": [
      "A√ß√µes estrat√©gicas para o primeiro ano"
    ]
  },
  "recursos_necessarios": {
    "investimento_inicial": "Investimento necess√°rio com justificativa",
    "equipe_recomendada": "Perfil da equipe necess√°ria",
    "tecnologias_essenciais": "Tecnologias que devem ser implementadas",
    "parcerias_estrategicas": "Parcerias que devem ser buscadas"
  },
  "validacao_dados": {
    "fontes_consultadas": "Lista das fontes consultadas via busca",
    "dados_validados": "Quais dados foram validados online",
    "informacoes_atualizadas": "Informa√ß√µes que foram atualizadas",
    "nivel_confianca": "N√≠vel de confian√ßa na an√°lise (0-100%)"
  }
}
```

## RELAT√ìRIO DE COLETA PARA AN√ÅLISE:
""",

            'deep_market_analysis': """
# ANALISTA DE MERCADO S√äNIOR - AN√ÅLISE PROFUNDA

Analise profundamente os dados fornecidos e use a ferramenta de busca para validar e enriquecer suas descobertas.

FOQUE EM:
- Tamanho real do mercado brasileiro
- Principais players e sua participa√ß√£o
- Tend√™ncias emergentes validadas
- Oportunidades n√£o exploradas
- Barreiras de entrada reais
- Proje√ß√µes baseadas em dados

Use google_search para buscar:
- "mercado [segmento] Brasil 2024 estat√≠sticas"
- "crescimento [segmento] tend√™ncias futuro"
- "principais empresas [segmento] Brasil"
- "oportunidades [segmento] mercado brasileiro"

DADOS PARA AN√ÅLISE:
""",

            'behavioral_analysis': """
# PSIC√ìLOGO COMPORTAMENTAL - AN√ÅLISE DE P√öBLICO

Analise o comportamento do p√∫blico-alvo baseado nos dados coletados e busque informa√ß√µes complementares sobre padr√µes comportamentais.

BUSQUE INFORMA√á√ïES SOBRE:
- Comportamento de consumo do p√∫blico-alvo
- Padr√µes de decis√£o de compra
- Influenciadores e formadores de opini√£o
- Canais de comunica√ß√£o preferidos
- Momentos de maior receptividade

Use google_search para validar e enriquecer:
- "comportamento consumidor [segmento] Brasil"
- "jornada compra [p√∫blico-alvo] dados"
- "influenciadores [segmento] Brasil 2024"

DADOS PARA AN√ÅLISE:
"""
        }

    def _create_deep_specialization_prompt(self, synthesis_type: str, full_context: str) -> str:
        """
        Cria prompt para ESPECIALIZA√á√ÉO PROFUNDA no material
        A IA deve se tornar um EXPERT no assunto espec√≠fico
        """
        
        # Extrair informa√ß√µes chave do contexto para personaliza√ß√£o
        context_preview = full_context[:2000]  # Primeiros 2000 chars para an√°lise
        
        base_specialization = f"""
üéì MISS√ÉO CR√çTICA: APRENDER PROFUNDAMENTE COM OS DADOS DA ETAPA 1

Voc√™ √© um CONSULTOR ESPECIALISTA que foi CONTRATADO por uma ag√™ncia de marketing.
Voc√™ recebeu um DOSSI√ä COMPLETO com dados reais coletados na Etapa 1.
Sua miss√£o √© APRENDER TUDO sobre este mercado espec√≠fico baseado APENAS nos dados fornecidos.

üìö PROCESSO DE APRENDIZADO OBRIGAT√ìRIO:

FASE 1 - ABSOR√á√ÉO TOTAL DOS DADOS (20-30 minutos):
- LEIA CADA PALAVRA dos dados fornecidos da Etapa 1
- MEMORIZE todos os nomes espec√≠ficos: influenciadores, marcas, produtos, canais
- ABSORVA todos os n√∫meros: seguidores, engajamento, pre√ßos, m√©tricas
- IDENTIFIQUE padr√µes √∫nicos nos dados coletados
- ENTENDA o comportamento espec√≠fico do p√∫blico encontrado nos dados
- APRENDA a linguagem espec√≠fica usada no nicho (baseada nos dados reais)

FASE 2 - APRENDIZADO T√âCNICO ESPEC√çFICO:
- Baseado nos dados, APRENDA as t√©cnicas mencionadas
- IDENTIFIQUE os principais players citados nos dados
- ENTENDA as tend√™ncias espec√≠ficas encontradas nos dados
- DOMINE os canais preferidos (baseado no que foi coletado)
- APRENDA sobre produtos/servi√ßos espec√≠ficos mencionados

FASE 3 - AN√ÅLISE COMERCIAL BASEADA NOS DADOS:
- IDENTIFIQUE oportunidades baseadas nos dados reais coletados
- MAPEIE concorrentes citados especificamente nos dados
- ENTENDA pricing mencionado nos dados
- ANALISE pontos de dor identificados nos dados
- PROJETE cen√°rios baseados nas tend√™ncias dos dados

FASE 4 - INSIGHTS EXCLUSIVOS DOS DADOS:
- EXTRAIA insights √∫nicos que APENAS estes dados espec√≠ficos revelam
- ENCONTRE oportunidades ocultas nos dados coletados
- DESENVOLVA estrat√©gias baseadas nos padr√µes encontrados
- PROPONHA solu√ß√µes baseadas nos problemas identificados nos dados

üéØ RESULTADO ESPERADO:
Uma an√°lise T√ÉO ESPEC√çFICA e BASEADA NOS DADOS que qualquer pessoa que ler vai dizer: 
"Nossa, essa pessoa estudou profundamente este mercado espec√≠fico!"

‚ö†Ô∏è REGRAS ABSOLUTAS - VOC√ä √â UM CONSULTOR PROFISSIONAL:
- VOC√ä FOI PAGO R$ 50.000 para se tornar EXPERT neste assunto espec√≠fico
- APENAS use informa√ß√µes dos dados fornecidos da Etapa 1
- CITE especificamente nomes, marcas, influenciadores encontrados nos dados
- MENCIONE n√∫meros exatos, m√©tricas, percentuais dos dados coletados
- REFERENCIE posts espec√≠ficos, v√≠deos, conte√∫dos encontrados nos dados
- GERE an√°lise EXTENSA (m√≠nimo 10.000 palavras) baseada no aprendizado
- SEMPRE indique de onde veio cada informa√ß√£o (qual dado da Etapa 1)
- TRATE como se sua carreira dependesse desta an√°lise

üìä DADOS DA ETAPA 1 PARA APRENDIZADO PROFUNDO:
{full_context}

üöÄ AGORA APRENDA PROFUNDAMENTE COM ESTES DADOS ESPEC√çFICOS!
TORNE-SE O MAIOR EXPERT NESTE MERCADO BASEADO NO QUE APRENDEU!

ESTRUTURA OBRIGAT√ìRIA DA AN√ÅLISE (CADA SE√á√ÉO DEVE TER PELO MENOS 3.000 PALAVRAS):

1. AN√ÅLISE DETALHADA DOS DADOS COLETADOS (3.000+ palavras)
   - Examine CADA dado espec√≠fico encontrado
   - Cite nomes, n√∫meros, m√©tricas exatas
   - Analise padr√µes e tend√™ncias

2. PERFIL COMPLETO DO P√öBLICO-ALVO (3.000+ palavras)
   - Baseado nos dados coletados
   - Demografia, comportamento, prefer√™ncias
   - Jornada do cliente espec√≠fica

3. AN√ÅLISE COMPETITIVA PROFUNDA (3.000+ palavras)
   - Todos os concorrentes mencionados nos dados
   - Estrat√©gias, pontos fortes e fracos
   - Oportunidades de diferencia√ß√£o

4. ESTRAT√âGIAS DE MARKETING ESPEC√çFICAS (3.000+ palavras)
   - Baseadas nos dados coletados
   - Canais, conte√∫dos, campanhas
   - M√©tricas e KPIs espec√≠ficos

5. OPORTUNIDADES DE NEG√ìCIO (3.000+ palavras)
   - Nichos descobertos nos dados
   - Produtos/servi√ßos potenciais
   - Proje√ß√µes e cen√°rios

6. PLANO DE A√á√ÉO DETALHADO (3.000+ palavras)
   - Passos espec√≠ficos e cronograma
   - Recursos necess√°rios
   - M√©tricas de sucesso

7. INSIGHTS EXCLUSIVOS E INOVA√á√ïES (3.000+ palavras)
   - Descobertas √∫nicas dos dados
   - Tend√™ncias emergentes
   - Oportunidades disruptivas

8. AN√ÅLISE DE RISCO E MITIGA√á√ÉO (3.000+ palavras)
   - Riscos identificados nos dados
   - Estrat√©gias de mitiga√ß√£o
   - Planos de conting√™ncia

TOTAL M√çNIMO: 24.000 PALAVRAS
SEJA EXTREMAMENTE DETALHADO EM CADA SE√á√ÉO!

INSTRU√á√ïES CR√çTICAS PARA GERAR CONTE√öDO EXTENSO:
- NUNCA pare de escrever at√© completar todas as 8 se√ß√µes
- CADA se√ß√£o deve ter pelo menos 3.000 palavras
- REPITA informa√ß√µes se necess√°rio para atingir o tamanho
- EXPANDA cada ponto com exemplos detalhados
- ADICIONE sub-se√ß√µes e sub-t√≥picos
- INCLUA listas extensas e detalhadas
- CITE dados espec√≠ficos repetidamente
- ELABORE cada insight com profundidade m√°xima
- CONTINUE escrevendo mesmo que pare√ßa repetitivo
- N√ÉO PARE at√© atingir 14.000+ palavras TOTAIS
"""

        return base_specialization

    async def execute_deep_specialization_study(
        self, 
        session_id: str,
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        EXECUTA ESTUDO PROFUNDO E ESPECIALIZA√á√ÉO COMPLETA NO MATERIAL
        
        A IA deve se tornar um ESPECIALISTA no assunto, estudando profundamente:
        - Todos os dados coletados (2MB+)
        - Padr√µes espec√≠ficos do mercado
        - Comportamentos √∫nicos do p√∫blico
        - Oportunidades comerciais detalhadas
        - Insights exclusivos e acion√°veis
        
        Args:
            session_id: ID da sess√£o
            synthesis_type: Tipo de especializa√ß√£o
        """
        logger.info(f"üéì INICIANDO ESTUDO PROFUNDO E ESPECIALIZA√á√ÉO para sess√£o: {session_id}")
        logger.info(f"üî• OBJETIVO: IA deve se tornar EXPERT no assunto para gerar 26 m√≥dulos robustos")
        
        try:
            # 1. CARREGAMENTO COMPLETO DOS DADOS REAIS
            logger.info("üìö FASE 1: Carregando TODOS os dados da Etapa 1...")
            consolidacao_data = self._load_consolidacao_etapa1(session_id)
            if not consolidacao_data:
                raise Exception("‚ùå CR√çTICO: Arquivo de consolida√ß√£o da Etapa 1 n√£o encontrado")
            
            viral_results_data = self._load_viral_results(session_id)
            viral_search_data = self._load_viral_search_completed(session_id)
            
            # 2. CONSTRU√á√ÉO DO CONTEXTO COMPLETO (SEM COMPRESS√ÉO)
            logger.info("üèóÔ∏è FASE 2: Construindo contexto COMPLETO sem compress√£o...")
            full_context = self._build_synthesis_context_from_json(
                consolidacao_data, viral_results_data, viral_search_data
            )
            
            context_size = len(full_context)
            logger.info(f"üìä Contexto constru√≠do: {context_size} chars (~{context_size//4} tokens)")
            
            if context_size < 500000:  # Menos de 500k chars
                logger.warning("‚ö†Ô∏è AVISO: Contexto pode ser insuficiente para especializa√ß√£o profunda")
            
            # 3. PROMPT DE ESPECIALIZA√á√ÉO PROFUNDA
            specialization_prompt = self._create_deep_specialization_prompt(synthesis_type, full_context)
            
            # 4. EXECU√á√ÉO DA ESPECIALIZA√á√ÉO (PROCESSO LONGO E DETALHADO)
            logger.info("üß† FASE 3: Executando ESPECIALIZA√á√ÉO PROFUNDA...")
            logger.info("‚è±Ô∏è Este processo pode levar 5-10 minutos para an√°lise completa")
            
            if not self.ai_manager:
                raise Exception("‚ùå AI Manager n√£o dispon√≠vel")
            
            # APRENDIZADO PROFUNDO COM OS DADOS REAIS DA ETAPA 1
            logger.info("üéì INICIANDO APRENDIZADO PROFUNDO COM DADOS REAIS...")
            logger.info("üìö IA vai APRENDER com todos os dados espec√≠ficos coletados")
            
            synthesis_result = await self.ai_manager.generate_with_active_search(
                prompt=specialization_prompt,
                context=full_context,
                session_id=session_id,
                max_search_iterations=15,  # M√ÅXIMO de buscas para aprendizado completo
                preferred_model="qwen",  # Usar modelo Sonoma Sky Alpha
                min_processing_time=300  # 10 minutos m√≠nimos para aprendizado profundo
            )
            
            # 6. Processa e valida resultado
            processed_synthesis = self._process_synthesis_result(synthesis_result)
            
            # 7. Salva s√≠ntese
            synthesis_path = self._save_synthesis_result(session_id, processed_synthesis, synthesis_type)
            
            # 8. Gera relat√≥rio de s√≠ntese
            synthesis_report = self._generate_synthesis_report(processed_synthesis, session_id)
            
            logger.info(f"‚úÖ S√≠ntese aprimorada conclu√≠da: {synthesis_path}")
            
            return {
                "success": True,
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "synthesis_path": synthesis_path,
                "synthesis_data": processed_synthesis,
                "synthesis_report": synthesis_report,
                "ai_searches_performed": self._count_ai_searches(synthesis_result),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese aprimorada: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    # Alias para manter compatibilidade
    async def execute_enhanced_synthesis(self, session_id: str, synthesis_type: str = "master_synthesis") -> Dict[str, Any]:
        """Alias para execute_deep_specialization_study - mant√©m compatibilidade"""
        return await self.execute_deep_specialization_study(session_id, synthesis_type)

    def _load_consolidacao_etapa1(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo consolidado.json da pesquisa web"""
        try:
            import os
            current_dir = os.getcwd()
            logger.info(f"üîç DEBUG: Diret√≥rio atual: {current_dir}")
            
            # Caminho correto: analyses_data/pesquisa_web/{session_id}/consolidado.json
            consolidado_path = Path(f"analyses_data/pesquisa_web/{session_id}/consolidado.json")
            absolute_path = consolidado_path.absolute()
            
            logger.info(f"üîç DEBUG: Caminho relativo: {consolidado_path}")
            logger.info(f"üîç DEBUG: Caminho absoluto: {absolute_path}")
            logger.info(f"üîç DEBUG: Arquivo existe: {consolidado_path.exists()}")
            
            if not consolidado_path.exists():
                logger.warning(f"‚ö†Ô∏è Arquivo consolidado n√£o encontrado: {consolidado_path}")
                return None
            
            # Carrega o arquivo consolidado.json
            with open(consolidado_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Consolida√ß√£o Etapa 1 carregada: {len(data.get('trechos', []))} trechos")
                return data
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar consolida√ß√£o Etapa 1: {e}")
            return None

    def _load_viral_results(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo viral_analysis_{session_id}_{timestamp}.json"""
        try:
            # Caminho correto: viral_data/
            viral_dir = Path("viral_data")
            
            if not viral_dir.exists():
                logger.warning(f"‚ö†Ô∏è Diret√≥rio viral_data n√£o encontrado")
                return None
            
            # Busca arquivo viral_analysis_{session_id}_*.json mais recente
            viral_files = list(viral_dir.glob(f"viral_analysis_{session_id}_*.json"))
            
            if not viral_files:
                logger.warning(f"‚ö†Ô∏è Arquivo viral_analysis para {session_id} n√£o encontrado em: {viral_dir}")
                return None
            
            # Pega o mais recente
            latest_file = max(viral_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Analysis encontrado: {latest_file}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar viral results: {e}")
            return None

    def _load_viral_search_completed(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega arquivo viral_search_completed_{timestamp}.json"""
        try:
            # Caminho: src/relatorios_intermediarios/workflow/{session_id}/
            workflow_dir = Path(f"relatorios_intermediarios/workflow/{session_id}")
            
            if not workflow_dir.exists():
                logger.warning(f"‚ö†Ô∏è Diret√≥rio workflow n√£o encontrado: {workflow_dir}")
                return None
            
            # Busca arquivo viral_search_completed_*.json
            viral_search_files = list(workflow_dir.glob("viral_search_completed_*.json"))
            
            if not viral_search_files:
                logger.warning(f"‚ö†Ô∏è Arquivo viral_search_completed n√£o encontrado em: {workflow_dir}")
                return None
            
            # Pega o mais recente
            latest_file = max(viral_search_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Search Completed encontrado: {latest_file}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar viral search completed: {e}")
            return None

    def _build_synthesis_context_from_json(
        self, 
        consolidacao_data: Dict[str, Any], 
        viral_results_data: Dict[str, Any] = None, 
        viral_search_data: Dict[str, Any] = None
    ) -> str:
        """Constr√≥i contexto COMPLETO para s√≠ntese a partir dos JSONs da Etapa 1 - SEM COMPRESS√ÉO"""
        
        context_parts = []
        
        # 1. Dados de consolida√ß√£o da Etapa 1 (COMPLETOS)
        if consolidacao_data:
            context_parts.append("# DADOS COMPLETOS DE CONSOLIDA√á√ÉO DA ETAPA 1")
            context_parts.append(json.dumps(consolidacao_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        # 2. Resultados virais (COMPLETOS)
        if viral_results_data:
            context_parts.append("# DADOS COMPLETOS DE AN√ÅLISE VIRAL")
            context_parts.append(json.dumps(viral_results_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        # 3. Busca viral completada (COMPLETOS)
        if viral_search_data:
            context_parts.append("# DADOS COMPLETOS DE BUSCA VIRAL COMPLETADA")
            context_parts.append(json.dumps(viral_search_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")
        
        full_context = "\n".join(context_parts)
        
        # Com Sonoma Sky Alpha (2M tokens), podemos usar dados completos!
        logger.info(f"üìä Contexto COMPLETO gerado: {len(full_context)} chars (~{len(full_context)//4} tokens)")
        logger.info(f"üî• Usando dados completos sem compress√£o - Modelo suporta 2M tokens!")
        
        return full_context

    def _build_synthesis_context(self, collection_report: str, viral_report: str = None) -> str:
        """Constr√≥i contexto completo para s√≠ntese (m√©todo legado)"""
        
        context = f"""
=== RELAT√ìRIO DE COLETA DE DADOS ===
{collection_report}
"""
        
        if viral_report:
            context += f"""

=== RELAT√ìRIO DE CONTE√öDO VIRAL ===
{viral_report}
"""
        
        context += f"""

=== INSTRU√á√ïES PARA S√çNTESE ===
- Analise TODOS os dados fornecidos acima
- Use a ferramenta google_search sempre que precisar de:
  * Dados mais recentes sobre o mercado
  * Valida√ß√£o de informa√ß√µes encontradas
  * Estat√≠sticas espec√≠ficas do Brasil
  * Tend√™ncias emergentes
  * Casos de sucesso documentados
  * Informa√ß√µes sobre concorr√™ncia

- Seja espec√≠fico e baseado em evid√™ncias
- Cite fontes quando poss√≠vel
- Foque no mercado brasileiro
- Priorize dados de 2024/2025
"""
        
        return context

    def _process_synthesis_result(self, synthesis_result: str) -> Dict[str, Any]:
        """Processa resultado da s√≠ntese"""
        try:
            # Tenta extrair JSON da resposta
            if "```json" in synthesis_result:
                start = synthesis_result.find("```json") + 7
                end = synthesis_result.rfind("```")
                json_text = synthesis_result[start:end].strip()
                
                parsed_data = json.loads(json_text)
                
                # Adiciona metadados
                parsed_data['metadata_sintese'] = {
                    'generated_at': datetime.now().isoformat(),
                    'engine': 'Enhanced Synthesis Engine v3.0',
                    'ai_searches_used': True,
                    'data_validation': 'REAL_DATA_ONLY',
                    'synthesis_quality': 'ULTRA_HIGH'
                }
                
                return parsed_data
            
            # Se n√£o encontrar JSON, tenta parsear a resposta inteira
            try:
                return json.loads(synthesis_result)
            except json.JSONDecodeError:
                # Fallback: cria estrutura b√°sica
                return self._create_enhanced_fallback_synthesis(synthesis_result)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar s√≠ntese: {e}")
            return self._create_enhanced_fallback_synthesis(synthesis_result)

    def _create_enhanced_fallback_synthesis(self, raw_text: str) -> Dict[str, Any]:
        """Cria s√≠ntese de fallback aprimorada"""
        return {
            "insights_principais": [
                "S√≠ntese gerada com dados reais coletados",
                "An√°lise baseada em fontes verificadas",
                "Informa√ß√µes validadas atrav√©s de busca ativa",
                "Dados espec√≠ficos do mercado brasileiro",
                "Tend√™ncias identificadas em tempo real"
            ],
            "oportunidades_identificadas": [
                "Oportunidades baseadas em dados reais do mercado",
                "Gaps identificados atrav√©s de an√°lise profunda",
                "Nichos descobertos via pesquisa ativa",
                "Tend√™ncias emergentes validadas online"
            ],
            "publico_alvo_refinado": {
                "demografia_detalhada": {
                    "idade_predominante": "Baseada em dados reais coletados",
                    "renda_familiar": "Validada com dados do IBGE",
                    "localizacao_geografica": "Concentra√ß√£o identificada nos dados"
                },
                "psicografia_profunda": {
                    "valores_principais": "Extra√≠dos da an√°lise comportamental",
                    "motivacoes_compra": "Identificadas nos dados sociais",
                    "influenciadores": "Mapeados atrav√©s da pesquisa"
                },
                "dores_viscerais_reais": [
                    "Dores identificadas atrav√©s de an√°lise real",
                    "Frustra√ß√µes documentadas nos dados coletados",
                    "Problemas validados via busca online"
                ],
                "desejos_ardentes_reais": [
                    "Aspira√ß√µes identificadas nos dados",
                    "Objetivos mapeados atrav√©s da pesquisa",
                    "Sonhos documentados no conte√∫do analisado"
                ]
            },
            "estrategias_recomendadas": [
                "Estrat√©gias baseadas em dados reais do mercado",
                "T√°ticas validadas atrav√©s de casos de sucesso",
                "Abordagens testadas no mercado brasileiro"
            ],
            "raw_synthesis": raw_text[:3000],
            "fallback_mode": True,
            "data_source": "REAL_DATA_COLLECTION",
            "timestamp": datetime.now().isoformat()
        }

    def _save_synthesis_result(
        self, 
        session_id: str, 
        synthesis_data: Dict[str, Any], 
        synthesis_type: str
    ) -> str:
        """Salva resultado da s√≠ntese"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            session_dir.mkdir(parents=True, exist_ok=True)
            
            # Salva JSON estruturado
            synthesis_path = session_dir / f"sintese_{synthesis_type}.json"
            with open(synthesis_path, 'w', encoding='utf-8') as f:
                json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            # Salva tamb√©m como resumo_sintese.json para compatibilidade
            if synthesis_type == 'master_synthesis':
                compat_path = session_dir / "resumo_sintese.json"
                with open(compat_path, 'w', encoding='utf-8') as f:
                    json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            return str(synthesis_path)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar s√≠ntese: {e}")
            raise

    def _generate_synthesis_report(
        self, 
        synthesis_data: Dict[str, Any], 
        session_id: str
    ) -> str:
        """Gera relat√≥rio leg√≠vel da s√≠ntese"""
        
        report = f"""# RELAT√ìRIO DE S√çNTESE - ARQV30 Enhanced v3.0

**Sess√£o:** {session_id}  
**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}  
**Engine:** Enhanced Synthesis Engine v3.0  
**Busca Ativa:** ‚úÖ Habilitada

---

## INSIGHTS PRINCIPAIS

"""
        
        # Adiciona insights principais
        insights = synthesis_data.get('insights_principais', [])
        for i, insight in enumerate(insights, 1):
            report += f"{i}. {insight}\n"
        
        report += "\n---\n\n## OPORTUNIDADES IDENTIFICADAS\n\n"
        
        # Adiciona oportunidades
        oportunidades = synthesis_data.get('oportunidades_identificadas', [])
        for i, oportunidade in enumerate(oportunidades, 1):
            report += f"**{i}.** {oportunidade}\n\n"
        
        # P√∫blico-alvo refinado
        publico = synthesis_data.get('publico_alvo_refinado', {})
        if publico:
            report += "---\n\n## P√öBLICO-ALVO REFINADO\n\n"
            
            # Demografia
            demo = publico.get('demografia_detalhada', {})
            if demo:
                report += "### Demografia Detalhada:\n"
                for key, value in demo.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"
            
            # Psicografia
            psico = publico.get('psicografia_profunda', {})
            if psico:
                report += "\n### Psicografia Profunda:\n"
                for key, value in psico.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"
            
            # Dores e desejos
            dores = publico.get('dores_viscerais_reais', [])
            if dores:
                report += "\n### Dores Viscerais Identificadas:\n"
                for i, dor in enumerate(dores[:10], 1):
                    report += f"{i}. {dor}\n"
            
            desejos = publico.get('desejos_ardentes_reais', [])
            if desejos:
                report += "\n### Desejos Ardentes Identificados:\n"
                for i, desejo in enumerate(desejos[:10], 1):
                    report += f"{i}. {desejo}\n"
        
        # Dados de mercado validados
        mercado = synthesis_data.get('dados_mercado_validados', {})
        if mercado:
            report += "\n---\n\n## DADOS DE MERCADO VALIDADOS\n\n"
            for key, value in mercado.items():
                label = key.replace('_', ' ').title()
                report += f"**{label}:** {value}\n\n"
        
        # Estrat√©gias recomendadas
        estrategias = synthesis_data.get('estrategias_recomendadas', [])
        if estrategias:
            report += "---\n\n## ESTRAT√âGIAS RECOMENDADAS\n\n"
            for i, estrategia in enumerate(estrategias, 1):
                report += f"**{i}.** {estrategia}\n\n"
        
        # Plano de a√ß√£o
        plano = synthesis_data.get('plano_acao_imediato', {})
        if plano:
            report += "---\n\n## PLANO DE A√á√ÉO IMEDIATO\n\n"
            
            if plano.get('primeiros_30_dias'):
                report += "### Primeiros 30 Dias:\n"
                for acao in plano['primeiros_30_dias']:
                    report += f"- {acao}\n"
            
            if plano.get('proximos_90_dias'):
                report += "\n### Pr√≥ximos 90 Dias:\n"
                for acao in plano['proximos_90_dias']:
                    report += f"- {acao}\n"
            
            if plano.get('primeiro_ano'):
                report += "\n### Primeiro Ano:\n"
                for acao in plano['primeiro_ano']:
                    report += f"- {acao}\n"
        
        # Valida√ß√£o de dados
        validacao = synthesis_data.get('validacao_dados', {})
        if validacao:
            report += "\n---\n\n## VALIDA√á√ÉO DE DADOS\n\n"
            report += f"**N√≠vel de Confian√ßa:** {validacao.get('nivel_confianca', 'N/A')}  \n"
            report += f"**Fontes Consultadas:** {len(validacao.get('fontes_consultadas', []))}  \n"
            report += f"**Dados Validados:** {validacao.get('dados_validados', 'N/A')}  \n"
        
        report += f"\n---\n\n*S√≠ntese gerada com busca ativa em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*"
        
        return report

    def _count_ai_searches(self, synthesis_text: str) -> int:
        """Conta quantas buscas a IA realizou"""
        if not synthesis_text:
            return 0
        
        try:
            # Conta men√ß√µes de busca no texto
            search_indicators = [
                'busca realizada', 'pesquisa online', 'dados encontrados',
                'informa√ß√µes atualizadas', 'valida√ß√£o online', 'google_search',
                'resultados da busca', 'pesquisa por', 'busquei por'
            ]
            
            count = 0
            text_lower = synthesis_text.lower()
            
            for indicator in search_indicators:
                count += text_lower.count(indicator)
            
            # Conta tamb√©m padr√µes de function calling
            import re
            function_calls = re.findall(r'google_search\(["\']([^"\']+)["\']\)', synthesis_text)
            count += len(function_calls)
            
            return count
        except Exception as e:
            logger.error(f"‚ùå Erro ao contar buscas da IA: {e}")
            return 0

    def get_synthesis_status(self, session_id: str) -> Dict[str, Any]:
        """Verifica status da s√≠ntese para uma sess√£o"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            
            # Verifica se existe s√≠ntese
            synthesis_files = list(session_dir.glob("sintese_*.json"))
            report_files = list(session_dir.glob("relatorio_sintese.md"))
            
            if synthesis_files or report_files:
                latest_synthesis = None
                if synthesis_files:
                    latest_synthesis = max(synthesis_files, key=lambda f: f.stat().st_mtime)
                
                return {
                    "status": "completed",
                    "synthesis_available": bool(synthesis_files),
                    "report_available": bool(report_files),
                    "latest_synthesis": str(latest_synthesis) if latest_synthesis else None,
                    "files_found": len(synthesis_files) + len(report_files)
                }
            else:
                return {
                    "status": "not_found",
                    "message": "S√≠ntese ainda n√£o foi executada"
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar status da s√≠ntese: {e}")
            return {"status": "error", "error": str(e)}
        

    async def execute_behavioral_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "behavioral_analysis")

    async def execute_market_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "deep_market_analysis")

# Inst√¢ncia global
enhanced_synthesis_engine = EnhancedSynthesisEngine()
